import streamlit as st
from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI 
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.core import Settings
import os

def get_huggingface_token():
    """ğŸ“Œ Hugging Face API í† í° ê°€ì ¸ì˜¤ê¸°"""
    # ì‹¤ì„œë²„ì—ì„œëŠ” osì˜ í™˜ê²½ë³€ìˆ˜ì— ì…‹íŒ…ëœë‹¤. ë”°ë¼ì„œ í™˜ê²½ë³€ìˆ˜ ì½ì–´ì˜¤ëŠ” ì½”ë“œë¡œ
    # ì‘ì„±í•´ì•¼ í•œë‹¤. 
    token  = os.environ.get("HUGGINGFACE_API_TOKEN") 
    # í† í°ì´ í™˜ê²¬ë³€ìˆ˜ì— ì—†ìœ¼ë©´, ë¡œì»¬ì—ì„œ ë™ì‘í•˜ë‹ˆê¹Œ ë¡œì»¬ì—ì„œ ì½ì–´ì˜¤ë„ë¡í•œë‹¤. 
    if token is None:
        token =  st.secrets.get("HUGGINGFACE_API_TOKEN")
    elif not token:
        st.error("ğŸš¨ HUGGINGFACE_API_TOKEN í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .streamlit/secrets.tomlì— ì¶”ê°€í•´ì£¼ì„¸ìš”.")
        return None
    return token

@st.cache_resource
def initialize_models():
    """ğŸ“Œ Hugging Face ê¸°ë°˜ ëª¨ë¸ ì´ˆê¸°í™”"""
    model_name = "mistralai/Mistral-7B-Instruct-v0.2"

    # âœ… í† í° ê°€ì ¸ì˜¤ê¸°
    token = get_huggingface_token()
    if not token:
        return None, None

    try:
        # âœ… Hugging Face Inference API ë¡œë“œ
        llm = HuggingFaceInferenceAPI(
            model_name=model_name,
            max_length=512,
            temperature=0.7,  # ğŸ”¹ ë³´ë‹¤ ìì—°ìŠ¤ëŸ¬ìš´ ì‘ë‹µì„ ìœ„í•´ 0.7ë¡œ ì„¤ì •
            messages=[
                {
                    "role": "system",
                    "content": "ë‹¹ì‹ ì€ í•œêµ­ì–´ë¡œ ëŒ€ë‹µí•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. "
                               "ì£¼ì–´ì§„ ì§ˆë¬¸ì— ëŒ€í•´ì„œë§Œ í•œêµ­ì–´ë¡œ ëª…í™•í•˜ê³  ì •í™•í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”. "
                               "ì‘ë‹µì˜ ë§ˆì§€ë§‰ ë¶€ë¶„ì€ ë‹¨ì–´ê°€ ì•„ë‹ˆë¼ ë¬¸ì¥ìœ¼ë¡œ ëë‚´ë„ë¡ í•´ì£¼ì„¸ìš”."
                }
            ],
            api_key=token  # ğŸ”¹ `token` ëŒ€ì‹  `api_key` ì‚¬ìš© (í˜¸í™˜ì„± ë¬¸ì œ í•´ê²°)
        )

        # âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
        embed_model = HuggingFaceEmbedding(model_name="sentence-transformers/all-MiniLM-L6-v2", token=token)
        
        # âœ… ê¸€ë¡œë²Œ ì„¤ì •
        Settings.llm = llm
        Settings.embed_model = embed_model

        return llm, embed_model

    except Exception as e:
        st.error(f"ğŸš¨ ëª¨ë¸ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return None, None

def main():
    """ğŸ“Œ Streamlit ì•± ì‹¤í–‰"""
    st.title('ğŸ“„ PDF ë¬¸ì„œ ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œ')
    st.write('ğŸ“ ì„ ì§„ê¸°ì—…ë³µì§€ ì—…ë¬´ë©”ë‰´ì–¼ì„ ê¸°ë°˜ìœ¼ë¡œ ì§ˆì˜ì‘ë‹µì„ ì œê³µí•©ë‹ˆë‹¤.')

    # âœ… ëª¨ë¸ ì´ˆê¸°í™”
    llm, embed_model = initialize_models()

    if llm and embed_model:
        st.success("âœ… ëª¨ë¸ì´ ì •ìƒì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤. ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”.")
        
        # âœ… ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°
        user_question = st.text_input("â“ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:", "")

        if user_question:
            with st.spinner("ğŸ” ë‹µë³€ì„ ìƒì„± ì¤‘..."):
                try:
                    response = llm.complete(prompt=user_question)
                    st.subheader("ğŸ’¬ AIì˜ ë‹µë³€")
                    st.write(response)
                except Exception as e:
                    st.error(f"ğŸš¨ ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

    else:
        st.warning("ğŸš¨ ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. API í† í°ì„ í™•ì¸í•˜ì„¸ìš”.")

if __name__ == '__main__':
    main()
